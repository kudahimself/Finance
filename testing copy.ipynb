{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650cab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas_ta\n",
    "import pandas_datareader.data as web\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27440956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_scraper import download_sp500_tickers \n",
    "sp500_list = download_sp500_tickers(save_local=True)\n",
    "\n",
    "from utils.data_scraper import download_sp500_data\n",
    "\n",
    "download_sp500_data(sp500_list, save_local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = utils.models\n",
    "data_scraper = utils.data_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62822d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_scraper.get_sp500_data()\n",
    "df.dropna(inplace=True)\n",
    "df = df[df.index.get_level_values('ticker') != 'SPY']\n",
    "sp500_list = data_scraper.get_sp500_tickers()\n",
    "sp500_list = [ticker for ticker in sp500_list if ticker != 'SPY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_bbands(series, length=20):\n",
    "\tbb = pandas_ta.bbands(close=np.log1p(series), length=length)\n",
    "\tif bb is not None:\n",
    "\t\treturn bb\n",
    "\t# Return a DataFrame of NaNs with the same index if bb is None\n",
    "\treturn pd.DataFrame(np.nan, index=series.index, columns=['BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0'])\n",
    "\n",
    "df['garman_klass_volatility'] = ((np.log(df['high'] - np.log(df['low']) ** 2))/2 - (2 * np.log(2) - 1) * (np.log(df['adj_close']) - np.log(df['open']) ** 2))\n",
    "df['rsi'] = df.groupby(level='ticker')['adj_close'].transform(lambda x: pandas_ta.rsi(x, length=20))\n",
    "df['bb_low'] = df.groupby(level='ticker')['adj_close'].transform(lambda x: safe_bbands(x, length=20).iloc[:,0])\n",
    "df['bb_mid'] = df.groupby(level='ticker')['adj_close'].transform(lambda x: safe_bbands(x, length=20).iloc[:,1])\n",
    "df['bb_high'] = df.groupby(level='ticker')['adj_close'].transform(lambda x: safe_bbands(x, length=20).iloc[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaaa37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 'close' prices for ticker 'AAPL' across all dates\n",
    "df.xs('AAPL', level='ticker')['rsi'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941959c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.xs('A', level='ticker').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_atr(stock_data: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    atr = pandas_ta.atr(high=stock_data['high'], low=stock_data['low'], close=stock_data['adj_close'], length=period)\n",
    "    return atr.sub(atr.mean()).div(atr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df['atr'] = df.groupby(level='ticker', group_keys=False).apply(compute_atr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_macd(close):\n",
    "    macd = pandas_ta.macd(close=close, length=20).iloc[:,0]\n",
    "    return macd.sub(macd.mean()).div(macd.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317dcab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['macd'] = df.groupby(level='ticker', group_keys=False)['adj_close'].apply(compute_macd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dollar_volume'] = (df['volume'] * df['adj_close'])/1e6\n",
    "df['close_over_open'] = np.log(df['adj_close'] / df['open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4419807",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols = [c for c in df.columns.unique(0) if c not in ['dollar_volume', 'volume', 'open', 'high', 'low', 'close']]\n",
    "data = pd.concat([df.unstack(level='ticker')['dollar_volume'].resample('ME').mean().stack(level='ticker', future_stack=True).to_frame('dollar_volume'),\n",
    "           df.unstack()[last_cols].resample('ME').last().stack(level='ticker', future_stack=True)], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f47cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dollar_volume'] = (data.loc[:,'dollar_volume'].unstack('ticker').rolling(5*12).mean().stack())\n",
    "data['dollar_vol_rank'] = data.groupby(level='date')['dollar_volume'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['dollar_vol_rank'] <= 150].drop(['dollar_volume', 'dollar_vol_rank'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eeff97",
   "metadata": {},
   "source": [
    "**Calculate Monthly Returns for different time horizons as features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_returns(df: pd.DataFrame, outlier_cutoffs: float = 0.005) -> pd.DataFrame:\n",
    "    lags = [1, 2, 3, 6, 9, 12]\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'return_{lag}m'] = (df['adj_close'].pct_change(lag)\n",
    "                                   .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoffs), upper=x.quantile(1 - outlier_cutoffs)))\n",
    "                                   .add(1)\n",
    "                                   .pow(1/lag)\n",
    "                                   .sub(1)\n",
    "                                   )\n",
    "    return df\n",
    "\n",
    "data = data.groupby(level='ticker', group_keys=False).apply(calculate_monthly_returns).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ca0ec",
   "metadata": {},
   "source": [
    "Download Fama-French Factors and Calculate Rolling Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.data_scraper.download_famafrench_data(save_local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9580766",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = data_scraper.get_famafrench_data()\n",
    "factor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_data = factor_data.join(data['return_1m']).sort_index()\n",
    "factor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14197a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = factor_data.groupby(level='ticker').size()\n",
    "valid_stocks = observations[observations >= 12]\n",
    "factor_data = factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]\n",
    "factor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e61d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import calculate_betas\n",
    "\n",
    "betas = calculate_betas(factor_data, method='rls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['mkt_rf', 'smb', 'hml', 'rmw', 'cma']\n",
    "group_data = data.join(betas.groupby(level='ticker').shift())\n",
    "group_data.loc[:, factors] = group_data.groupby(level='ticker', group_keys=False)[factors].apply(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a30f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = group_data.dropna()\n",
    "group_data = group_data.drop('adj_close', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e260c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'rsi', 'garman_klass_volatility',# 'close_over_open', #'atr', 'macd', 'bb_low', 'bb_mid', 'bb_high',\n",
    "    'mkt_rf', 'smb', 'hml', 'rmw', 'cma'\n",
    "]\n",
    "cols = features + ['target_1m']\n",
    "feature_data = group_data.copy()\n",
    "feature_data['target_1m'] = feature_data.groupby('ticker')['return_1m'].shift(-1)\n",
    "# convert infinities to NaN first (they also cause dropna)\n",
    "feature_data = feature_data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "null_counts = feature_data[cols].isnull().sum().sort_values(ascending=False)\n",
    "null_counts_percent = (feature_data[cols].isnull().mean() * 100).round(2)\n",
    "print(\"Null counts:\")\n",
    "print(null_counts)\n",
    "print(\"\\nPercent nulls:\")\n",
    "print(null_counts_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [\n",
    "#     'rsi', 'garman_klass_volatility',# 'close_over_open', #'atr', 'macd', 'bb_low', 'bb_mid', 'bb_high',\n",
    "#     'mkt_rf', 'smb', 'hml', 'rmw', 'cma'\n",
    "# ]\n",
    "# cols = features + ['target_1m']\n",
    "# feature_data = group_data.copy()\n",
    "# feature_data['target_1m'] = feature_data.groupby('ticker')['return_1m'].shift(-1)\n",
    "# model_df = feature_data.reset_index().dropna(subset=features + ['target_1m'])\n",
    "# dates = sorted(model_df['date'].unique())\n",
    "# for d in dates:\n",
    "#     print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare supervised target and features (assumes `data` exists)\n",
    "feature_data = group_data.copy()\n",
    "feature_data['target_1m'] = feature_data.groupby('ticker')['return_1m'].shift(-1)\n",
    "\n",
    "features = [\n",
    "    'rsi', 'garman_klass_volatility', 'close_over_open',\n",
    "    'atr', 'macd', 'bb_low', 'bb_mid', 'bb_high',\n",
    "    'return_2m', 'return_3m', 'return_6m',\n",
    "    'mkt_rf', 'smb', 'hml', 'rmw', 'cma'\n",
    "]\n",
    "\n",
    "# features = [\n",
    "#     'rsi', 'garman_klass_volatility', 'atr', 'macd',\n",
    "#     'bb_low', 'bb_mid', 'bb_high',\n",
    "#     'return_1m', 'mkt_rf', 'smb', 'hml', 'rmw', 'cma'\n",
    "# ]\n",
    "\n",
    "model_df = feature_data.reset_index().dropna(subset=features + ['target_1m'])\n",
    "\n",
    "from utils.models import rolling_train_predict_windowed\n",
    "# Run the windowed rolling trainer\n",
    "fixed_dates_pred, preds_df, last_model, scaler = rolling_train_predict_windowed(\n",
    "    model_df, features,\n",
    "    top_k=30, model_type='randomforest', window_months=12, min_train_rows=100\n",
    ")\n",
    "\n",
    "# Use this mapping for downstream optimisation / backtest\n",
    "# Offset the dates by 1 day so they start at the beginning of each month\n",
    "fixed_dates = {}\n",
    "for d, tickers in fixed_dates_pred.items():\n",
    "    # Parse date, add 1 day, then set to first day of the month\n",
    "    new_date = (pd.to_datetime(d) + pd.DateOffset(days=1)).replace(day=1)\n",
    "    fixed_dates[new_date.strftime('%Y-%m-%d')] = tickers\n",
    "print(f'Created fixed_dates for {len(fixed_dates)} months')\n",
    "\n",
    "\n",
    "# Optional: show last model interpretability\n",
    "if last_model is not None:\n",
    "    try:\n",
    "        if hasattr(last_model, 'coef_'):\n",
    "            coeffs = pd.Series(last_model.coef_, index=features).sort_values(ascending=False)\n",
    "            print('Ridge coefficients (last model):')\n",
    "            print(coeffs)\n",
    "        elif hasattr(last_model, 'feature_importances_'):\n",
    "            imps = pd.Series(last_model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "            print('Feature importances (last model):')\n",
    "            print(imps)\n",
    "    except Exception as e:\n",
    "        print('Model inspection error:', e)\n",
    "else:\n",
    "    print('No model trained (insufficient history).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add next month predictions\n",
    "future_investment_data = group_data.copy()\n",
    "future_investment = future_investment_data.drop('return_1m', axis=1).groupby('ticker')\n",
    "\n",
    "# print('feature_data info', feature_data.info())\n",
    "future_model_df = future_investment_data.reset_index().dropna(subset=features)\n",
    "\n",
    "last_date = future_model_df['date'].max()\n",
    "print('last_date', last_date)\n",
    "future_date = last_date + pd.DateOffset(days=1)\n",
    "print(future_date)\n",
    "pool = future_model_df[future_model_df['date'] == last_date].dropna(subset=features).copy()\n",
    "\n",
    "X_pred = scaler.transform(pool[features])\n",
    "y_pred = last_model.predict(X_pred)\n",
    "\n",
    "pool = pool.assign(y_pred=y_pred)\n",
    "\n",
    "selected = pool[pool['y_pred'] > 0].nlargest(10, 'y_pred')\n",
    "fixed_dates[future_date.strftime('%Y-%m-%d')] = selected['ticker'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97aa714",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beefcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = feature_data.index.get_level_values('ticker').unique().tolist()\n",
    "\n",
    "new_df = data_scraper.get_sp500_data(tickers=stocks,\n",
    "                                     start_date=feature_data.index.get_level_values('date').unique()[0]-pd.DateOffset(months=12),\n",
    "                                     end_date=feature_data.index.get_level_values('date').unique()[-1])\n",
    "new_df = new_df.adj_close.unstack('ticker')\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfcc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.portfolio import optimise_weights\n",
    "\n",
    "returns_dataframe = np.log(new_df).diff().dropna()\n",
    "\n",
    "portfolio_df = pd.DataFrame()\n",
    "\n",
    "for start_date in fixed_dates.keys():\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        end_date = (pd.to_datetime(start_date) + pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        cols = fixed_dates[start_date]\n",
    "        \n",
    "        optimisation_start_date = (pd.to_datetime(start_date) - pd.DateOffset(months=12)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        optimisation_end_date = (pd.to_datetime(start_date) - pd.DateOffset(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        optimisation_df = new_df.loc[optimisation_start_date:optimisation_end_date]\n",
    "\n",
    "        success = False\n",
    "        try:\n",
    "            \n",
    "            weights = optimise_weights(prices=optimisation_df,\n",
    "                                lower_bound=round(1/len(optimisation_df.columns)*0.5, 3))\n",
    "            \n",
    "            weights = pd.DataFrame(weights, index=pd.Series(0))\n",
    "            \n",
    "            success = True\n",
    "        except Exception as e:\n",
    "            weights = pd.DataFrame()\n",
    "            print(f\"Maximium Sharpe Ratio optimization failed for start date {start_date}: {e} continuing with equal weights for all tickers\")\n",
    "        \n",
    "        \n",
    "        if success == False:\n",
    "            weights = pd.DataFrame([1/len(optimisation_df.columns) for i in range(len(optimisation_df.columns))],\n",
    "                                   index=optimisation_df.columns.tolist(),\n",
    "                                   columns=pd.Series(0)).T\n",
    "        temp_df = returns_dataframe[start_date:end_date]\n",
    "        \n",
    "        temp_df = temp_df.stack(future_stack=True).to_frame('return').reset_index(level=0) \\\n",
    "            .merge(weights.stack(future_stack=True).to_frame('weight').reset_index(level=0, drop=True),\n",
    "                left_index=True,\n",
    "                    right_index=True) \\\n",
    "            .reset_index().set_index(['date', 'ticker']).unstack().stack(future_stack=True)\n",
    "            \n",
    "        temp_df.index.names = ['date' , 'ticker']\n",
    "        \n",
    "        temp_df['weighted_return'] = temp_df['return'] * temp_df['weight']\n",
    "        \n",
    "        temp_df = temp_df.groupby(level='date')['weighted_return'].sum().to_frame('strategy_return')\n",
    "        \n",
    "        portfolio_df = pd.concat([portfolio_df, temp_df])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for start date {start_date}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_ret = data_scraper.get_sp500_data(tickers=['SPY'],\n",
    "                                     start_date='2022-02-01',\n",
    "                                     end_date='2025-11-01')\n",
    "spy_ret = spy_ret.reset_index().drop('ticker', axis=1)\n",
    "spy_ret['spy_ret'] = np.log(spy_ret['adj_close']).diff()\n",
    "spy_ret = spy_ret[['date', 'spy_ret']]\n",
    "spy_ret = spy_ret.set_index('date').dropna()\n",
    "portfolio_df = portfolio_df.merge(spy_ret, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df.to_csv('data/portfolio_returns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "portfolio_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum())-1\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "portfolio_cumulative_return.plot(ax=ax)\n",
    "plt.title(\"Cumulative Returns: RSI and Technical Indicators (BB, Volatility, MACD, ATR) vs SPY\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
